# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
"""Workflows for estimating and correcting head motion in ASL images."""

from nipype.interfaces import ants, fsl
from nipype.interfaces import utility as niu
from nipype.pipeline import engine as pe
from niworkflows.engine.workflows import LiterateWorkflow as Workflow
from niworkflows.interfaces.images import RobustAverage
from niworkflows.interfaces.itk import MCFLIRT2ITK
from niworkflows.utils.connections import listify

from aslprep import config
from aslprep.interfaces.confounds import CreateFakeMotionOutputs, NormalizeMotionParams
from aslprep.interfaces.reference import SelectHighestContrastVolumes
from aslprep.interfaces.utility import (
    C3dAffineToolFix,
    CombineMotionParameters,
    CombineMotions,
    ConcatITK,
    PairwiseRMSDiff,
    Smooth,
    SplitByVolumeType,
    TSplit,
)


def init_asl_hmc_wf(
    use_ge,
    mem_gb,
    omp_nthreads,
    name='asl_hmc_wf',
):
    """Estimate head-motion parameters and optionally correct them for intensity differences.

    This workflow separately estimates motion parameters for each unique type of volume
    (e.g., control, label, deltam, M0, CBF), and then stitches the resulting parameters
    back together according to the aslcontext file.

    Workflow Graph
        .. workflow::
            :graph2use: orig
            :simple_form: yes

            from aslprep.workflows.asl.hmc import init_asl_hmc_wf

            wf = init_asl_hmc_wf(
                mem_gb=3,
                omp_nthreads=1,
                name="asl_hmc_wf",
            )

    Parameters
    ----------
    use_ge : :obj:`bool`
        Disable HMC for GE data.
    mem_gb : :obj:`float`
        Size of ASL file in GB
    omp_nthreads : :obj:`int`
        Maximum number of threads an individual process may use
    name : :obj:`str`
        Name of workflow (default: ``asl_hmc_wf``)

    Inputs
    ------
    asl_file
        Control-label pair series NIfTI file.
        If an ASL run contains M0 volumes, deltaM volumes, or CBF volumes,
        those volumes should be removed before running this workflow.
    aslcontext
        ASL context TSV file.
    raw_ref_image
        Reference image to which ASL series is motion corrected

    Outputs
    -------
    xforms
        ITKTransform file aligning each volume to ``ref_image``
    movpar_file
        MCFLIRT motion parameters, normalized to SPM format (X, Y, Z, Rx, Ry, Rz)
    rmsd_file
        Framewise displacement as measured by ``fsl_motion_outliers``

    Notes
    -----
    ASLPrep uses volume type-wise motion correction :footcite:p:`wang2008empirical` instead of the
    zig-zag regression approach :footcite:p:`wang2012improving` because it is unclear how
    M0 volumes should be treated in the zig-zag method.

    References
    ----------
    .. footbibliography::
    """
    workflow = Workflow(name=name)

    inputnode = pe.Node(
        niu.IdentityInterface(
            fields=[
                'asl_file',
                'aslcontext',
                'processing_target',
                'raw_ref_image',
            ],
        ),
        name='inputnode',
    )

    outputnode = pe.Node(
        niu.IdentityInterface(
            fields=[
                'movpar_file',
                'xforms',
                'rmsd_file',
            ],
        ),
        name='outputnode',
    )

    if use_ge:
        # Skip HMC, just mock up movpar and xform files with identity transforms
        workflow.__desc__ = """\
Head-motion correction was not performed on the ASL data.
Instead, a temporary reference image was generated by averaging the highest contrast volumes
within the ASL series.
This was then registered to the ASL reference image.
This single transform was then duplicated for each volume in the ASL series.

"""
        select_highest_contrast_volumes = pe.Node(
            SelectHighestContrastVolumes(prioritize_m0=use_ge),
            name='select_highest_contrast_volumes',
            mem_gb=1,
        )
        workflow.connect([
            (inputnode, select_highest_contrast_volumes, [
                ('asl_file', 'asl_file'),
                ('aslcontext', 'aslcontext'),
            ]),
        ])  # fmt:skip

        gen_avg = pe.Node(RobustAverage(), name='gen_avg', mem_gb=1)
        workflow.connect([
            (select_highest_contrast_volumes, gen_avg, [('selected_volumes_file', 'in_file')]),
        ])  # fmt:skip

        reference_buffer = pe.Node(
            niu.IdentityInterface(fields=['aslref']),
            name='reference_buffer',
        )

        if config.workflow.smooth_kernel > 0:
            # Smooth the reference image to match the aslref processing
            workflow.__desc__ += (
                'The reference image was then smoothed with a Gaussian kernel '
                f'(FWHM = {config.workflow.smooth_kernel} mm).'
            )
            smooth_reference = pe.Node(
                Smooth(fwhm=config.workflow.smooth_kernel),
                name='smooth_reference',
                mem_gb=config.DEFAULT_MEMORY_MIN_GB,
            )
            workflow.connect([
                (gen_avg, smooth_reference, [('out_file', 'in_file')]),
                (smooth_reference, reference_buffer, [('out_file', 'aslref')]),
            ])  # fmt:skip
        else:
            workflow.connect([(gen_avg, reference_buffer, [('out_file', 'aslref')])])

        # Now register the temporary reference image to the ASL reference image
        register_reference = pe.Node(
            ants.Registration(
                transforms=['Rigid'],
                metric=['Mattes'],
                metric_weight=[1],
                shrink_factors=[[4, 2, 1]],
                smoothing_sigmas=[[2, 1, 0]],
                number_of_iterations=[[10000, 10000, 10000]],
                convergence_threshold=[1e-08, 1e-08, 1e-08],
                convergence_window_size=[20, 20, 20],
                write_composite_transform=False,
                use_histogram_matching=True,
                transform_parameters=[[0.1]],
            ),
            name='register_reference',
        )
        workflow.connect([
            (reference_buffer, register_reference, [('aslref', 'fixed_image')]),
            (inputnode, register_reference, [('raw_ref_image', 'moving_image')]),
        ])  # fmt:skip

        create_fake_motion_outputs = pe.Node(
            CreateFakeMotionOutputs(),
            name='create_fake_motion_outputs',
        )
        workflow.connect([
            (inputnode, create_fake_motion_outputs, [('asl_file', 'asl_file')]),
            (register_reference, create_fake_motion_outputs, [('forward_transforms', 'xform')]),
            (create_fake_motion_outputs, outputnode, [
                ('movpar_file', 'movpar_file'),
                ('xforms', 'xforms'),
                ('rmsd_file', 'rmsd_file'),
            ]),
        ])  # fmt:skip
        return workflow

    # Otherwise, perform ASL-specific HMC
    workflow.__desc__ = """\
Head-motion parameters were estimated for the ASL data using *FSL*'s `mcflirt` [@mcflirt].
Motion correction was performed separately for each of the volume types
in order to account for intensity differences between different contrasts,
which, when motion corrected together, can conflate intensity differences with
head motions [@wang2008empirical].
Next, ASLPrep concatenated the motion parameters across volume types and
re-calculated relative root mean-squared deviation.

"""

    split_by_volume_type = pe.Node(
        SplitByVolumeType(),
        name='split_by_volume_type',
    )
    workflow.connect([
        (inputnode, split_by_volume_type, [
            ('aslcontext', 'aslcontext'),
            ('asl_file', 'asl_file'),
        ]),
    ])  # fmt:skip

    mcflirt = pe.MapNode(
        fsl.MCFLIRT(save_mats=True, save_plots=True, save_rms=False),
        name='mcflirt',
        mem_gb=mem_gb * 3,
        iterfield=['in_file'],
    )
    workflow.connect([
        (inputnode, mcflirt, [('raw_ref_image', 'ref_file')]),
        (split_by_volume_type, mcflirt, [('out_files', 'in_file')]),
    ])  # fmt:skip

    listify_mat_files = pe.MapNode(
        niu.Function(
            function=listify,
            input_names=['value'],
            output_names=['lst'],
        ),
        name='listify_mat_files',
        iterfield=['value'],
    )
    workflow.connect([(mcflirt, listify_mat_files, [('mat_file', 'value')])])

    # Combine the motpars files, mat files, and rms files across the different MCFLIRTed files,
    # based on the aslcontext file.
    combine_motpars = pe.Node(
        CombineMotionParameters(),
        name='combine_motpars',
    )
    workflow.connect([
        (inputnode, combine_motpars, [('aslcontext', 'aslcontext')]),
        (split_by_volume_type, combine_motpars, [('volume_types', 'volume_types')]),
        (mcflirt, combine_motpars, [('par_file', 'par_files')]),
        (listify_mat_files, combine_motpars, [('lst', 'mat_files')]),
    ])  # fmt:skip

    # Use rmsdiff to calculate relative rms from transform files.
    rmsdiff = pe.Node(PairwiseRMSDiff(), name='rmsdiff')

    workflow.connect([
        (inputnode, rmsdiff, [('raw_ref_image', 'ref_file')]),
        (combine_motpars, rmsdiff, [('mat_file_list', 'in_files')]),
        (rmsdiff, outputnode, [('out_file', 'rmsd_file')]),
    ])  # fmt:skip

    fsl2itk = pe.Node(MCFLIRT2ITK(), name='fsl2itk', mem_gb=0.05, n_procs=omp_nthreads)

    workflow.connect([
        (inputnode, fsl2itk, [
            ('raw_ref_image', 'in_source'),
            ('raw_ref_image', 'in_reference'),
        ]),
        (combine_motpars, fsl2itk, [('mat_file_list', 'in_files')]),
        (fsl2itk, outputnode, [('out_file', 'xforms')]),
    ])  # fmt:skip

    normalize_motion = pe.Node(
        NormalizeMotionParams(format='FSL'),
        name='normalize_motion',
        mem_gb=config.DEFAULT_MEMORY_MIN_GB,
    )
    workflow.connect([
        (combine_motpars, normalize_motion, [('combined_par_file', 'in_file')]),
        (normalize_motion, outputnode, [('out_file', 'movpar_file')]),
    ])  # fmt:skip

    return workflow


def init_linear_alignment_wf(mem_gb=1, omp_nthreads=1, name='linear_alignment_wf'):
    """Align each of a set of input images to a target image with ANTS.

    This effectively mimics MCFLIRT with a more robust registration method.
    """
    workflow = Workflow(name=name)
    inputnode = pe.Node(
        niu.IdentityInterface(fields=['asl_file', 'aslcontext', 'raw_ref_image']),
        name='inputnode',
    )
    outputnode = pe.Node(
        niu.IdentityInterface(
            fields=[
                'registered_image_paths',
                'xforms',
                'movpar_file',
                'rmsd_file',
            ],
        ),
        name='outputnode',
    )
    split_asl = pe.Node(TSplit(), name='split_asl')
    workflow.connect([(inputnode, split_asl, [('asl_file', 'in_file')])])

    reg = ants.Registration(
        dimension=3,
        float=True,
        winsorize_lower_quantile=0.002,
        winsorize_upper_quantile=0.998,
        collapse_output_transforms=True,
        write_composite_transform=False,
        use_histogram_matching=[False],
        transforms=['Rigid'],
        number_of_iterations=[[1000, 1000]],
        output_warped_image=True,
        transform_parameters=[[0.2], [0.15]],
        convergence_threshold=[1e-08, 1e-08],
        convergence_window_size=[20, 20],
        metric=['Mattes'],
        sampling_percentage=[0.15],
        sampling_strategy=['Random'],
        smoothing_sigmas=[[8.0, 2.0]],
        sigma_units=['mm'],
        metric_weight=[1.0],
        shrink_factors=[[2, 1]],
        radius_or_number_of_bins=[48],
        interpolation='BSpline',
        num_threads=omp_nthreads,
    )
    iter_reg = pe.MapNode(
        reg,
        name='reg',
        iterfield=['moving_image'],
        n_procs=omp_nthreads,
        mem_gb=mem_gb,
    )
    workflow.connect([
        (inputnode, iter_reg, [('raw_ref_image', 'fixed_image')]),
        (split_asl, iter_reg, [('out_files', 'moving_image')]),
    ])  # fmt:skip

    flatten_xforms = pe.Node(
        niu.Function(
            function=flatten_list_of_lists,
            input_names=['in_list'],
            output_names=['out_list'],
        ),
        name='flatten_xforms',
    )
    workflow.connect([(iter_reg, flatten_xforms, [('forward_transforms', 'in_list')])])

    concat_xforms = pe.Node(
        ConcatITK(),
        name='concat_xforms',
    )
    workflow.connect([
        (flatten_xforms, concat_xforms, [('out_list', 'inlist')]),
        (concat_xforms, outputnode, [('xforms', 'xforms')]),
    ])  # fmt:skip

    itk2fsl = pe.MapNode(
        C3dAffineToolFix(ras2fsl=True),
        name='itk2fsl',
        iterfield=['in_itk'],
        n_procs=omp_nthreads,
    )
    workflow.connect([
        (inputnode, itk2fsl, [
            # use the same image as both source and reference
            # see https://github.com/PennLINC/qsiprep/pull/301
            ('raw_ref_image', 'reference_file'),
            ('raw_ref_image', 'source_file'),
        ]),
        (flatten_xforms, itk2fsl, [('out_list', 'in_itk')]),
    ])  # fmt:skip

    # Use rmsdiff to calculate relative rms from transform files.
    rmsdiff = pe.Node(PairwiseRMSDiff(), name='rmsdiff')

    workflow.connect([
        (inputnode, rmsdiff, [('raw_ref_image', 'ref_file')]),
        (itk2fsl, rmsdiff, [('out_file', 'in_files')]),
        (rmsdiff, outputnode, [('out_file', 'rmsd_file')]),
    ])  # fmt:skip

    combine_motions = pe.Node(
        CombineMotions(),
        name='combine_motions',
    )
    workflow.connect([
        (inputnode, combine_motions, [('raw_ref_image', 'ref_file')]),
        (itk2fsl, combine_motions, [('out_file', 'transform_files')]),
        (combine_motions, outputnode, [('confounds_file', 'movpar_file')]),
    ])  # fmt:skip

    return workflow


def flatten_list_of_lists(in_list):
    """Flatten a list of lists."""
    return [item for sublist in in_list for item in sublist]
